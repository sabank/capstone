show4calls
The goal of this milestone report is just to display that we've gotten used to
working with the data and that we are on track to create our prediction algorithm.
library(tm)
install.packages(tm,RWeka,wordcloud)
install.packages(tm,RWeka)
install.packages(tm)
install.packages("tm")
install.packages("RWeka","wordcloud")
install.packages("RWeka")
install.packages("wordcloud")
install.packages("Java")
install.packages(c("wordcloud","tm"),repos="http://cran.r-project.org")
library(wordcloud)
library(tm)
setwd("/Volumes/My Passport Ultra/3-Documents/34-EDUCATION/342_Coursera/JohnsHopkinsUniversity/GitHub/capstone")
blogs <- "./final/en_US/en_US.blogs.txt"
news <- "./final/en_US/en_US.news.txt"
twitter <- "./final/en_US/en_US.twitter.txt"
library(stringi)
library(tm);library(RWeka)
library(ggplot2)
library(wordcloud)
# read files
r_blogs <- readLines(blogs, encoding = "UTF-8", skipNul=TRUE)
r_news <- readLines(news, encoding = "UTF-8", skipNul=TRUE)
r_twitter <- readLines(twitter, encoding = "UTF-8", skipNul=TRUE)
# file size
s_blogs <- utils:::format.object_size(file.info(blogs)$size,"auto")
s_news <- utils:::format.object_size(file.info(news)$size,"auto")
s_twitter <- utils:::format.object_size(file.info(twitter)$size,"auto")
# line count
l_blogs <- length(r_blogs)
l_news <- length(r_news)
l_twitter <- length(r_twitter)
# word count
w_blogs <- sum(stri_count_words(r_blogs))
w_news <- sum(stri_count_words(r_news))
w_twitter <- sum(stri_count_words(r_twitter))
# summary
filesummary <- data.frame(FileName = c("Blogs","News","Twitter"),
FileSize = c(s_blogs,s_news,s_twitter),
LineCount = c(l_blogs,l_news,l_twitter),
WordCount = c(w_blogs,w_news,w_twitter))
colnames(filesummary) <- c(" File Name "," File Size "," Total Line "," Total Word ")
filesummary
save.image("/Volumes/My Passport Ultra/3-Documents/34-EDUCATION/342_Coursera/JohnsHopkinsUniversity/GitHub/capstone/Untitled.RData")
set.seed(987)
# sampling
sp_blogs <- sample(r_blogs,l_blogs * 0.002)
sp_news <- sample(r_news,l_news * 0.002)
sp_twitter <- sample(r_twitter,l_twitter * 0.002)
sp_data <- c(sp_blogs,sp_news,sp_twitter)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
save.image("/Volumes/My Passport Ultra/3-Documents/34-EDUCATION/342_Coursera/JohnsHopkinsUniversity/GitHub/capstone/Untitled.RData")
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- tm_map(sp_dataclean,removeWords,profanityWords)
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,removeWords,profanityWords)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
# sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
sp_dataclean <- tm_map(sp_dataclean,PlainTextDocument)
sp_corpus <- data.frame(text=unlist(sapply(sp_dataclean,`[`,"content")),stringsAsFactors=FALSE)
View(sp_corpus)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation))
?tm_map
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation,preserve_intra_word_dashes = TRUE))
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation),preserve_intra_word_dashes = TRUE)
sp_dataclean <- tm_map(sp_dataclean,PlainTextDocument)
sp_dataclean <- tm_map(sp_dataclean,removeWords,profanityWords)
sp_dataclean <- VCorpus(VectorSource(sp_data))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(function(x) iconv(x,to="UTF-8",sub="byte")),mc.cores=2)
sp_dataclean <- tm_map(sp_dataclean,removeWords,stopwords("english"))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(tolower),lazy=TRUE)
sp_dataclean <- tm_map(sp_dataclean,stripWhitespace)
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeURL))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removeNumbers))
sp_dataclean <- tm_map(sp_dataclean,content_transformer(removePunctuation),preserve_intra_word_dashes=TRUE)
sp_dataclean <- tm_map(sp_dataclean,PlainTextDocument)
sp_corpus <- data.frame(text=unlist(sapply(sp_dataclean,`[`,"content")),stringsAsFactors=FALSE)
View(sp_corpus)
getNGram <- function(corpus, mingrams, maxgrams) {
ngram <- NGramTokenizer(corpus, Weka_control(min=mingrams,max=maxgrams,delimiters=" \\r\\n\\t.,;:\"()?!"))
ngram <- data.frame(table(ngram))
ngram <- ngram[order(ngram$Freq,decreasing=TRUE),][1:10,]
colnames(ngram) <- c("String","Count")
ngram
}
uniGram <- getNGram(sp_corpus,1,1)
View(uniGram)
biGram  <- getNGram(sp_corpus,2,2)
triGram <- getNGram(sp_corpus,3,3)
View(biGram)
View(uniGram)
View(triGram)
par(mfrow = c(1, 3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2')
)
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(4,4))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2')
)
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.65,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=100,
random.order=FALSE,
rot.per=0.65,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=100,
random.order=FALSE,
rot.per=0.65,
colors=brewer.pal(20, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=100,
random.order=FALSE,
rot.per=0.65,
colors=brewer.pal(3, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.15,
colors=brewer.pal(4, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.75,
colors=brewer.pal(4, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.95,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
max.words=200,
random.order=TRUE,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'))
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
View(uniGram)
View(biGram)
View(triGram)
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'),)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
rot.per=0.35,
colors=brewer.pal(8, 'Dark2'),
ordered.colors = TRUE)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
rot.per=0.35,
ordered.colors = TRUE)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
rot.per=0,
colors = brewer.pal(8,"Accent")
ordered.colors = TRUE)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per=0,
colors=brewer.pal(8,'Accent'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per=0,
colors=brewer.pal(7,'Accent'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per=0,
colors=brewer.pal(12,'Paired'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per=0,
colors=brewer.pal(12,'Set3'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
use.r.layout = TRUE,
colors=brewer.pal(12,'Set3'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
use.r.layout = TRUE,
colors=brewer.pal(12,'Paired'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per = .35
use.r.layout = TRUE,
colors=brewer.pal(12,'Paired'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
par(mfrow = c(1,3))
createWC <- function(ngram, nlabel) {
wordcloud(ngram$String,
ngram$Count,
random.order=FALSE,
rot.per = .35,
use.r.layout = TRUE,
colors=brewer.pal(12,'Paired'))
text(x=0.5, y=-0.1, nlabel)
}
createWC(uniGram, "Uni-Gram Wordcloud")
createWC(biGram, "Bi-Gram Wordcloud")
createWC(triGram, "Tri-Gram Wordcloud")
setwd("~/Documents/capstone")
setwd("/Volumes/My Passport Ultra/3-Documents/34-EDUCATION/342_Coursera/JohnsHopkinsUniversity/GitHub/capstone")
blogs <- "./final/en_US/en_US.blogs.txt"
news <- "./final/en_US/en_US.news.txt"
twitter <- "./final/en_US/en_US.twitter.txt"
library(stringi)
library(tm);library(RWeka)
library(ggplot2)
library(wordcloud)
r_blogs <- readLines(blogs, encoding = "UTF-8", skipNul=TRUE)
r_news <- readLines(news, encoding = "UTF-8", skipNul=TRUE)
r_twitter <- readLines(twitter, encoding = "UTF-8", skipNul=TRUE)
# file size
s_blogs <- utils:::format.object_size(file.info(blogs)$size,"auto")
s_news <- utils:::format.object_size(file.info(news)$size,"auto")
s_twitter <- utils:::format.object_size(file.info(twitter)$size,"auto")
# line count
l_blogs <- length(r_blogs)
l_news <- length(r_news)
l_twitter <- length(r_twitter)
# word count
w_blogs <- stri_count_words(r_blogs)
w_news <- stri_count_words(r_news)
w_twitter <- stri_count_words(r_twitter)
# summary
filesummary <- data.frame(FileName = c("Blogs","News","Twitter"),
FileSize = c(s_blogs,s_news,s_twitter),
LineCount = c(l_blogs,l_news,l_twitter),
WordCount = c(w_blogs,w_news,w_twitter))
colnames(filesummary) <- c("FileName","FileSize","TotalLine","TotalWord")
ggplot(filesummary, aes(x=TotalWord)) + geom_histogram(binwidth=.5, colour="black", fill="white") +
facet_grid(FileName ~ .)
ggplot(filesummary, aes(x=log(TotalWord))) + geom_histogram(binwidth=.5, colour="black", fill=FileName) +
facet_grid(FileName ~ .)
ggplot(filesummary, aes(x=log(TotalWord),fill=FileName)) + geom_histogram(binwidth=.5, colour="black") +
facet_grid(FileName ~ .)
ggplot(filesummary, aes(x=log(TotalWord),fill=FileName)) + geom_histogram(binwidth=.2, colour="black") +
facet_grid(FileName ~ .)
View(filesummary)
ggplot(filesummary, aes(x=FileName, y=TotalWord, fill=FileName)) + geom_boxplot()
ggplot(filesummary, aes(x=FileName, y=log(TotalWord), fill=FileName)) + geom_boxplot()
ggplot(filesummary, aes(x=FileName, y=log(TotalWord), fill=FileName)) + geom_boxplot() + stat_summary(fun.y=mean, geom="point", shape=5, size=4)
ggplot(filesummary, aes(x=FileName, y=log(TotalWord), fill=FileName)) + geom_boxplot()
